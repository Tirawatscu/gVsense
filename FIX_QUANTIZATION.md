# Data Loss Fix: Timestamp Quantization

## üîç Root Cause Discovered

**NOT blocking commands** - the issue is **timestamp quantization collisions!**

### The Problem

At 100Hz sampling with 10ms quantization:
1. Samples arrive every **10ms** (1000ms / 100Hz = 10ms)
2. Timestamps are quantized to **10ms boundaries** (0, 10, 20, 30, ...)
3. Due to timing jitter, adjacent samples sometimes round to the **SAME timestamp**
4. InfluxDB overwrites the earlier sample with the later one
5. Result: Data loss!

### Example

```
Sample 1: arrives at 1000.008ms ‚Üí quantized to 1000ms ‚Üí saved
Sample 2: arrives at 1000.018ms ‚Üí quantized to 1000ms ‚Üí OVERWRITES Sample 1!
```

### Your Data Confirms This

- **gVs002**: 1 sample lost = 1 timestamp collision
- **gVs003**: 10 samples lost = 10 timestamp collisions

## ‚úÖ The Solution

**Change quantization from 10ms to 1ms or 5ms**

### Option 1: 1ms Quantization (Recommended)
- Each sample gets unique timestamp
- No collisions possible
- Best precision

### Option 2: 5ms Quantization
- Each sample gets unique timestamp
- No collisions possible
- Cleaner timestamp values

## üöÄ How to Fix

### Method 1: Web UI (if available)
1. Open web interface
2. Go to settings
3. Change "Timestamp Quantization" from 10ms to 1ms
4. Restart acquisition

### Method 2: Configuration File
Edit your config file:
```json
{
  "timestamp_quantization_ms": 1
}
```

### Method 3: Python Code
```python
# In web_server.py or your main script
config['timestamp_quantization_ms'] = 1

# Or programmatically:
if seismic and hasattr(seismic, 'timing_adapter'):
    seismic.timing_adapter.timestamp_generator.set_quantization(1)
```

### Method 4: Direct Command
```bash
# Modify config
python3 -c "
import json
with open('config.json', 'r') as f:
    cfg = json.load(f)
cfg['device']['timestamp_quantization_ms'] = 1
with open('config.json', 'w') as f:
    json.dump(cfg, f, indent=2)
print('‚úÖ Quantization set to 1ms')
"
```

## üìä Expected Results

### Before Fix (10ms quantization)
- Data loss: 0.003-0.028%
- Lost samples: 1-10 per 36,000
- Cause: Timestamp collisions

### After Fix (1ms quantization)
- Data loss: 0%
- Lost samples: 0
- Cause: None - each sample has unique timestamp

## üî¨ Verification

After changing to 1ms quantization, run for 6 minutes and check:

```python
from investigate_quantization_loss import analyze_your_data_loss

# With 1ms quantization
analyze_your_data_loss(quantization_ms=1)
```

Expected result: **0 data loss**

## ‚ö†Ô∏è Safe Quantization Values

For 100Hz sampling (10ms interval):

‚úÖ **SAFE** (no collisions):
- **1ms** - Best precision, guaranteed unique
- **2ms** - Good precision
- **5ms** - Clean numbers, still unique

‚ùå **UNSAFE** (causes collisions):
- **10ms** - ~0.003-0.028% loss (your current issue)
- **20ms** - ~50% loss (catastrophic!)
- **50ms** - ~80% loss (catastrophic!)
- **100ms** - ~90% loss (catastrophic!)

## üéØ Summary

**Problem**: 10ms quantization = 10ms sampling interval ‚Üí timestamp collisions ‚Üí data loss

**Solution**: Use 1ms quantization ‚Üí unique timestamps ‚Üí zero data loss

**Implementation**: Change one configuration value from 10 to 1

That's it! üéâ

